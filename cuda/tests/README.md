# Test implementations of streamed feed_forward and normalize_layer modules 
## Dependencies:
1. Python -> 3.7.10
2. pytorch+cuda -> 1.8.0+cu101
3. pandas 
4. json.hpp -> ("https://github.com/nlohmann/json")
## Json dumps:
The json dumps have json files corresponding to bert parameters that can be directly loaded, into the checking script and the normalize_layer.cu.
The files inside dump are:
1. encoder.layer.0.output.LayerNorm.weight.json (768 dim weight vector for layer_norm step for the first encoder layer)
2. encoder.layer.0.output.LayerNorm.bias.json (768 dim bias vector for layer_norm step for the first encoder layer)
3. bert_norm.json (generated from colab: it is the output of the input norm layer for the input of all ones, with 4 batch size, 512 seq length and 768 hidden size)
4. input_norm.json (to be generated by the NormalizeLayer executable)
## Running instructions
(Please make sure that the json.hpp is present inside the tests folder)
### For running normalize_layer.cu

compile the cuda source

```nvcc -std=c++11 normalize_layer.cu -lcublas -o NormalizeLayer```

run the binary file 

```./NormalizeLayer 4 512 768 4```

(the params are: <batch_size> <sequence_length> <hidden_size> <num_queues>)
the output is dumped to "input_norm.json"

```python compare_outputs.py 1e-5```

The filenames are hardcoded inside the script. The epsilon is a command line argument. 

### For running normalize_layer.cu

compile the cuda source

```nvcc -std=c++11 feed_forward.cu -lcublas -o FeedForward```

run the binary file 

```./FeedForward 4 512 768 4```

(the params are: <batch_size> <sequence_length> <hidden_size> <num_queues>)
validation part TBA
<!-- the output is dumped to "input_norm.json"
```python compare_outputs.py input_norm.json bert_norm.json 4 512 768 4``` -->